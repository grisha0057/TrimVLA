#!/bin/bash

# ========== æ¸²æŸ“é…ç½® ==========
export MUJOCO_GL=osmesa
export PYOPENGL_PLATFORM=osmesa

# ========== Python è¾“å‡ºé…ç½® ==========
# ç¦ç”¨ Python è¾“å‡ºç¼“å†²ï¼Œè®©æ—¥å¿—å®æ—¶æ˜¾ç¤º
export PYTHONUNBUFFERED=1

# ========== PyTorch ä¼˜åŒ–é…ç½® ==========
# å¯ç”¨ TF32 åŠ é€Ÿï¼ˆAmpereåŠä»¥ä¸Šæ¶æ„ï¼‰
export TORCH_ALLOW_TF32_CUBLAS_OVERRIDE=1
export TORCH_CUDNN_V8_API_ENABLED=1

# ========== è·¯å¾„é…ç½® ==========
VLA_PATH=${VLA_PATH:-"/root/workspace/LightVLA/checkpoints/openvla-libero-spatial"}
DATA_ROOT_DIR=${DATA_ROOT_DIR:-"/root/workspace/LightVLA/datasets/rlds/modified_libero_rlds_full"}
DATASET_NAME=${DATASET_NAME:-"libero_spatial_no_noops"}
RUN_ROOT_DIR=${RUN_ROOT_DIR:-"/root/workspace/LightVLA/logs/libero_spatial_training"}
EXPERIMENT_NAME=${EXPERIMENT_NAME:-"libero_spatial_$(date +%Y%m%d_%H%M%S)"}

# ========== è®­ç»ƒè¶…å‚æ•°ï¼ˆä¸ overfit å®éªŒä¿æŒä¸€è‡´ï¼‰==========
# å­¦ä¹ ç‡ä¸è°ƒåº¦
LR=${LR:-1e-4}
MAX_STEPS=${MAX_STEPS:-1400}
WARMUP_STEPS=${WARMUP_STEPS:-0}
DECAY_MILESTONES=${DECAY_MILESTONES:-"[100000]"}  # åŸºæœ¬ä¸è¡°å‡
DECAY_GAMMA=${DECAY_GAMMA:-0.5}

# æ‰¹æ¬¡ä¸æ¢¯åº¦ç´¯ç§¯ï¼ˆä¼˜åŒ–åï¼šæå‡GPUåˆ©ç”¨ç‡ï¼‰
BATCH_SIZE=${BATCH_SIZE:-4}
GRAD_ACCUMULATION=${GRAD_ACCUMULATION:-8}

# LoRA é…ç½®ï¼ˆä¸ overfit ä¸€è‡´ï¼‰
LORA_RANK=${LORA_RANK:-8}

# ä¿å­˜ç­–ç•¥
SAVE_FREQ=${SAVE_FREQ:-200}
SAVE_LATEST_ONLY=${SAVE_LATEST_ONLY:-False}

IMAGE_AUG=${IMAGE_AUG:-True}

# ç¦ç”¨ç­›é€‰
PRUNE_DISABLE=${PRUNE_DISABLE:-False}

# Coverage å‚æ•°ï¼ˆå‰¯æ—‹é’®ï¼‰ï¼šè·Ÿéšæœ€å°ä¿ç•™
COVERAGE_WARMUP=${COVERAGE_WARMUP:-1.0}
PRUNE_COVERAGE_FOLLOW_MIN_KEEP=${PRUNE_COVERAGE_FOLLOW_MIN_KEEP:-True}
PRUNE_COVERAGE_OFFSET=${PRUNE_COVERAGE_OFFSET:-0.05}

# èšåˆæ–¹å¼ï¼šlogsumexpï¼ˆæ¨èï¼‰| mean | max
PRUNE_AGGREGATION=${PRUNE_AGGREGATION:-"logsumexp"}
PRUNE_LSE_TEMP=${PRUNE_LSE_TEMP:-1.0}      # LogSumExp æ¸©åº¦å‚æ•°

# Soft rescale å‚æ•°
PRUNE_RESCALE=${PRUNE_RESCALE:-True}        # å¯ç”¨å‡å€¼ä¿æŒçš„ rescale
PRUNE_CLIP=${PRUNE_CLIP:-10.0}             # Rescale è£å‰ªé˜ˆå€¼

# ST-TopK è®­ç»ƒï¼ˆGumbel-Softmax + ç›´é€šï¼‰
PRUNE_TRAIN_USE_ST_TOPK=${PRUNE_TRAIN_USE_ST_TOPK:-True}
PRUNE_TAU_START=${PRUNE_TAU_START:-1.0}
PRUNE_TAU_END=${PRUNE_TAU_END:-0.30}
PRUNE_TAU_RAMP_STEPS=${PRUNE_TAU_RAMP_STEPS:-1200}

# æœ€å°ä¿ç•™æ¯”ä¾‹ï¼ˆä¸»æ—‹é’®ï¼‰
PRUNE_DISABLE_KEEP_BINS=${PRUNE_DISABLE_KEEP_BINS:-True}
PRUNE_MIN_KEEP_RATIO_WARMUP=${PRUNE_MIN_KEEP_RATIO_WARMUP:-1.0}
PRUNE_MIN_KEEP_RATIO_TARGET=${PRUNE_MIN_KEEP_RATIO_TARGET:-0.20}
PRUNE_MIN_KEEP_RAMP_STEPS=${PRUNE_MIN_KEEP_RAMP_STEPS:-1200}

# ========== è¯„ä¼°é…ç½® ==========
EVAL_NUM_TRIALS=${EVAL_NUM_TRIALS:-3}       # æ¯ä¸ªä»»åŠ¡è¯„ä¼°3æ¬¡ï¼ˆå¿«é€Ÿè¯„ä¼°ï¼‰
EVAL_GPUS=${EVAL_GPUS:-"0,1"}               # è¯„ä¼°ä½¿ç”¨çš„GPU

# ========== åˆ†å¸ƒå¼è®­ç»ƒé…ç½® ==========
# GPU é…ç½®
NPROC=${NPROC:-2}
CUDA_DEVICES=${CUDA_VISIBLE_DEVICES:-"0,1"}
export CUDA_VISIBLE_DEVICES=${CUDA_DEVICES}

# ç½‘ç»œé…ç½®
export MASTER_ADDR=${MASTER_ADDR:-127.0.0.1}
export MASTER_PORT=${MASTER_PORT:-29500}


# ========== å‰ç½®æ£€æŸ¥ ==========
echo "============================================"
echo "ğŸš€ LightVLA - LIBERO Spatial è®­ç»ƒ+è¯„ä¼°"
echo "============================================"
echo ""
echo "ğŸ“Š è®­ç»ƒé…ç½®ï¼ˆä¼˜åŒ–ç‰ˆï¼šæå‡è®­ç»ƒé€Ÿåº¦ï¼‰ï¼š"
echo "  - æ¨¡å‹è·¯å¾„: ${VLA_PATH}"
echo "  - æ•°æ®è·¯å¾„: ${DATA_ROOT_DIR}"
echo "  - æ•°æ®é›†åç§°: ${DATASET_NAME}"
echo "  - å®éªŒåç§°: ${EXPERIMENT_NAME}"
echo ""
echo "ğŸ¯ è®­ç»ƒè¶…å‚æ•°ï¼š"
echo "  - å­¦ä¹ ç‡: ${LR}"
echo "  - æ€»æ­¥æ•°: ${MAX_STEPS}"
echo "  - ä¿å­˜é¢‘ç‡: æ¯ ${SAVE_FREQ} æ­¥"
echo "  - Warmup: ${WARMUP_STEPS} æ­¥"
echo "  - æ‰¹æ¬¡å¤§å°: ${BATCH_SIZE}"
echo "  - æ¢¯åº¦ç´¯ç§¯: ${GRAD_ACCUMULATION}"
echo "  - LoRA Rank: ${LORA_RANK}"
echo "  - å›¾åƒå¢å¼º: ${IMAGE_AUG}"
echo ""
echo "ğŸ” è§†è§‰ Token ç­›é€‰ï¼š"
echo "  - å¯ç”¨: $([ "${PRUNE_DISABLE}" = "False" ] && echo 'âœ…' || echo 'âŒ')"
echo "  - æœ€å°ä¿ç•™æ¯”ä¾‹: ${PRUNE_MIN_KEEP_RATIO_WARMUP} -> ${PRUNE_MIN_KEEP_RATIO_TARGET} (ramp ${PRUNE_MIN_KEEP_RAMP_STEPS})"
echo "  - Coverage: è·Ÿéš min_keep_ratio (follow_min_keep=${PRUNE_COVERAGE_FOLLOW_MIN_KEEP}, +${PRUNE_COVERAGE_OFFSET})"
echo "  - é‡åŒ–æ¡¶: $([ "${PRUNE_DISABLE_KEEP_BINS}" = "True" ] && echo 'ç¦ç”¨' || echo 'å¯ç”¨')"
echo "  - Gumbel æ¸©åº¦: ${PRUNE_TAU_START} -> ${PRUNE_TAU_END} (ramp ${PRUNE_TAU_RAMP_STEPS})"
echo "  - èšåˆæ–¹å¼: ${PRUNE_AGGREGATION}"
echo ""
echo "ğŸ® è¯„ä¼°é…ç½®ï¼š"
echo "  - è¯„ä¼°é¢‘ç‡: æ¯ ${SAVE_FREQ} æ­¥"
echo "  - æ¯ä»»åŠ¡è¯•éªŒæ¬¡æ•°: ${EVAL_NUM_TRIALS}"
echo "  - è¯„ä¼°GPU: ${EVAL_GPUS}"
echo ""
echo "ğŸ–¥ï¸  è®­ç»ƒGPU: ${CUDA_DEVICES} (${NPROC}å¡)"
echo ""

# æ£€æŸ¥æ¨¡å‹
if [ ! -d "${VLA_PATH}" ]; then
    echo "âŒ é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: ${VLA_PATH}"
    exit 1
fi

# æ£€æŸ¥æ•°æ®é›†
if [ ! -d "${DATA_ROOT_DIR}/${DATASET_NAME}" ]; then
    echo "âŒ é”™è¯¯: æ•°æ®é›†ä¸å­˜åœ¨: ${DATA_ROOT_DIR}/${DATASET_NAME}"
    exit 1
fi

# æ£€æŸ¥ dataset_info.json
DATASET_INFO="${DATA_ROOT_DIR}/${DATASET_NAME}/1.0.0/dataset_info.json"
if [ -f "${DATASET_INFO}" ]; then
    echo "âœ… æ•°æ®é›†ä¿¡æ¯ï¼š"
    if command -v jq >/dev/null 2>&1; then
        NUM_SHARDS=$(jq '.splits[0].shardLengths | length' "${DATASET_INFO}")
        SHARD_LENGTH=$(jq '.splits[0].shardLengths[0]' "${DATASET_INFO}")
        TOTAL_SAMPLES=$((NUM_SHARDS * SHARD_LENGTH))
        NUM_BYTES=$(jq -r '.splits[0].numBytes' "${DATASET_INFO}")
        echo "  - åˆ†ç‰‡æ•°: ${NUM_SHARDS}"
        echo "  - æ¯åˆ†ç‰‡æ ·æœ¬: ${SHARD_LENGTH}"
        echo "  - æ€»æ ·æœ¬æ•°: ${TOTAL_SAMPLES}"
        echo "  - æ•°æ®å¤§å°: $((NUM_BYTES / 1024 / 1024)) MB"
    fi
fi

echo ""
echo "âœ… æ£€æŸ¥é€šè¿‡"
echo ""

# ========== åˆ›å»ºå®éªŒç›®å½• ==========
EXPERIMENT_DIR="${RUN_ROOT_DIR}/${EXPERIMENT_NAME}"
mkdir -p ${EXPERIMENT_DIR}
LOG_FILE="${EXPERIMENT_DIR}/train.log"
EVAL_LOG_FILE="${EXPERIMENT_DIR}/eval_results.log"

# å›ºå®š run_idï¼Œé¿å…ç»­è®­æ—¶åç§°è¶Šæ¥è¶Šé•¿ï¼›ä¸ finetune çš„ --run_id_override å¯¹é½
RUN_ID_FIXED="${EXPERIMENT_NAME}"

# ä¿å­˜é…ç½®
cat > ${EXPERIMENT_DIR}/config.txt <<EOF
è®­ç»ƒ+è¯„ä¼°é…ç½® - $(date)
==================
æ¨¡å‹: ${VLA_PATH}
æ•°æ®é›†: ${DATA_ROOT_DIR}/${DATASET_NAME}
å®éªŒåç§°: ${EXPERIMENT_NAME}

è®­ç»ƒè¶…å‚æ•°:
  å­¦ä¹ ç‡: ${LR}
  æ€»æ­¥æ•°: ${MAX_STEPS}
  ä¿å­˜é¢‘ç‡: ${SAVE_FREQ}
  Warmup: ${WARMUP_STEPS}
  æ‰¹æ¬¡å¤§å°: ${BATCH_SIZE}
  æ¢¯åº¦ç´¯ç§¯: ${GRAD_ACCUMULATION}
  LoRA Rank: ${LORA_RANK}
  å›¾åƒå¢å¼º: ${IMAGE_AUG}

è§†è§‰ Token ç­›é€‰:
  ç¦ç”¨: ${PRUNE_DISABLE}
  Coverage: è·Ÿéš min_keep_ratio (+${PRUNE_COVERAGE_OFFSET})
  èšåˆ: ${PRUNE_AGGREGATION}
  æ¸©åº¦: ${PRUNE_LSE_TEMP}
  Rescale: ${PRUNE_RESCALE}

è¯„ä¼°é…ç½®:
  é¢‘ç‡: æ¯ ${SAVE_FREQ} æ­¥
  æ¯ä»»åŠ¡è¯•éªŒ: ${EVAL_NUM_TRIALS}
  è¯„ä¼°GPU: ${EVAL_GPUS}

è®­ç»ƒGPU: ${CUDA_DEVICES}
EOF

# ========== ä¿®å¤ä¸»æœºåè§£æ ==========
HN=$(hostname)
if ! grep -q "\b${HN}\b" /etc/hosts 2>/dev/null; then
  echo "ğŸ§© ä¿®å¤ä¸»æœºåè§£æ..."
  {
    echo "127.0.0.1 ${HN}"
  } >> /etc/hosts 2>/dev/null || echo "âš ï¸ æ— æ³•ä¿®æ”¹ /etc/hosts"
fi

# ========== å®šä¹‰è¯„ä¼°å‡½æ•° ==========
run_evaluation() {
    local checkpoint_path=$1
    local step=$2
    
    # æ ¹æ®å½“å‰ step è®¡ç®—è¯„æµ‹ç”¨çš„ min_keep_ratio ä¸ coverage
    local ratio_warm=${PRUNE_MIN_KEEP_RATIO_WARMUP}
    local ratio_tgt=${PRUNE_MIN_KEEP_RATIO_TARGET}
    local ratio_ramp=${PRUNE_MIN_KEEP_RAMP_STEPS}
    local cov_follow=${PRUNE_COVERAGE_FOLLOW_MIN_KEEP}
    local cov_off=${PRUNE_COVERAGE_OFFSET}
    local ratio
    if [ ${step} -lt ${WARMUP_STEPS} ]; then
        ratio=${ratio_warm}
    else
        # çº¿æ€§æ’å€¼ï¼šä» warmup æ­¥å¼€å§‹åˆ° ramp å®Œæˆ
        local passed=$(( step - WARMUP_STEPS ))
        if [ ${passed} -lt 0 ]; then passed=0; fi
        if [ ${ratio_ramp} -le 0 ]; then
            ratio=${ratio_tgt}
        else
            # clamp åˆ° [0,1]
            local num=$(python - <<PY
passed=${passed}
ratio_ramp=${ratio_ramp}
print(min(1.0, max(0.0, passed/ratio_ramp)))
PY
)
            ratio=$(python - <<PY
rw=${ratio_warm}
rt=${ratio_tgt}
p=${num}
print((1.0-p)*rw + p*rt)
PY
)
        fi
    fi
    local cov
    if [ "${cov_follow}" = "True" ]; then
        cov=$(python - <<PY
ratio=${ratio}
off=${cov_off}
print(min(0.999, ratio+off))
PY
)
    else
        echo "âŒ é”™è¯¯: PRUNE_COVERAGE_FOLLOW_MIN_KEEP å¿…é¡»ä¸º True"
        exit 1
    fi
    
    echo ""
    echo "============================================"
    echo "ğŸ® å¼€å§‹è¯„ä¼° Checkpoint: ${checkpoint_path}"
    echo "   Step: ${step} | min_keep_ratio=${ratio} | coverage=${cov}"
    echo "============================================"
    
    # ä¿å­˜å½“å‰ CUDA_VISIBLE_DEVICES
    local TRAIN_CUDA_DEVICES=${CUDA_VISIBLE_DEVICES}
    
    # è®¾ç½®è¯„ä¼°ç”¨çš„GPU
    export CUDA_VISIBLE_DEVICES=${EVAL_GPUS}
    
    # è¿è¡Œè¯„ä¼°
    local eval_start_time=$(date +%s)
    
    cd /root/workspace/LightVLA
    
    # ä½¿ç”¨ python -u å¯ç”¨unbufferedè¾“å‡ºï¼Œé€šè¿‡ tee åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
    python -u experiments/robot/libero/run_libero_eval.py \
        --pretrained_checkpoint "${checkpoint_path}" \
        --task_suite_name "libero_spatial" \
        --use_l1_regression True \
        --use_diffusion False \
        --use_film False \
        --num_images_in_input 2 \
        --use_proprio True \
        --lora_rank ${LORA_RANK} \
        --center_crop True \
        --num_trials_per_task ${EVAL_NUM_TRIALS} \
        --prune_disable_keep_bins ${PRUNE_DISABLE_KEEP_BINS} \
        --prune_min_keep_ratio ${ratio} \
        --prune_coverage_follow_min_keep ${PRUNE_COVERAGE_FOLLOW_MIN_KEEP} \
        --prune_coverage_offset ${PRUNE_COVERAGE_OFFSET} \
        --run_id_note "step_${step}" \
        --local_log_dir "${EXPERIMENT_DIR}/eval_logs" \
        --save_rollout_video False \
        --seed 7 2>&1 | tee -a ${EVAL_LOG_FILE}
    
    local eval_exit_code=$?
    local eval_end_time=$(date +%s)
    local eval_duration=$((eval_end_time - eval_start_time))
    
    # æ¢å¤è®­ç»ƒç”¨çš„GPUè®¾ç½®
    export CUDA_VISIBLE_DEVICES=${TRAIN_CUDA_DEVICES}
    
    if [ ${eval_exit_code} -eq 0 ]; then
        echo "âœ… è¯„ä¼°å®Œæˆ (è€—æ—¶: ${eval_duration}ç§’)" | tee -a ${EVAL_LOG_FILE}
    else
        echo "âŒ è¯„ä¼°å¤±è´¥ (exit code: ${eval_exit_code})" | tee -a ${EVAL_LOG_FILE}
    fi
    
    echo "============================================"
    echo ""
}

# ========== åˆ†é˜¶æ®µè®­ç»ƒ+è¯„ä¼° ==========
cd /root/workspace/LightVLA

# è®¡ç®—éœ€è¦è®­ç»ƒçš„é˜¶æ®µæ•°
NUM_STAGES=$((MAX_STEPS / SAVE_FREQ))

echo "ğŸ“ è®­ç»ƒè®¡åˆ’: æ€»å…± ${MAX_STEPS} æ­¥ï¼Œåˆ† ${NUM_STAGES} ä¸ªé˜¶æ®µ"
echo "   æ¯é˜¶æ®µ ${SAVE_FREQ} æ­¥åè¿›è¡Œè¯„ä¼°"
echo ""

# ========== Step 0: è¯„ä¼°åˆå§‹æ¨¡å‹ï¼ˆå·²è·³è¿‡ï¼‰==========
echo "============================================"
echo "â­ï¸  Step 0: è·³è¿‡åˆå§‹æ¨¡å‹è¯„ä¼°ï¼ˆä¹‹å‰å·²è¯„æµ‹ï¼‰"
echo "============================================"
echo ""

CURRENT_STEP=0
LAST_CHECKPOINT_PATH="${VLA_PATH}"

for stage in $(seq 1 ${NUM_STAGES}); do
    TARGET_STEP=$((stage * SAVE_FREQ))
    STEPS_THIS_STAGE=$((TARGET_STEP - CURRENT_STEP))
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»­è®­
    if [ ${stage} -eq 1 ]; then
        # ç¬¬ä¸€é˜¶æ®µï¼šä»å¤´å¼€å§‹è®­ç»ƒ
        RESUME_FLAG="False"
        RESUME_STEP_ARG=""
        TRAIN_MODE="ä»å¤´è®­ç»ƒ"
    else
        # åç»­é˜¶æ®µï¼šä»ä¸Šä¸€ä¸ªcheckpointç»§ç»­è®­ç»ƒ
        RESUME_FLAG="True"
        RESUME_STEP_ARG="--resume_step ${CURRENT_STEP}"
        TRAIN_MODE="ç»­è®­ï¼ˆä»Step ${CURRENT_STEP}ç»§ç»­ï¼‰"
    fi
    
    echo ""
    echo "============================================"
    echo "ğŸƒ é˜¶æ®µ ${stage}/${NUM_STAGES}: è®­ç»ƒè‡³ ${TARGET_STEP} æ­¥"
    echo "   å½“å‰æ­¥æ•°: ${CURRENT_STEP}"
    echo "   æœ¬é˜¶æ®µè®­ç»ƒ: ${STEPS_THIS_STAGE} æ­¥"
    echo "   è®­ç»ƒæ¨¡å¼: ${TRAIN_MODE}"
    echo "   Checkpoint: ${LAST_CHECKPOINT_PATH}"
    echo "============================================"
    echo ""
    
    # è®­ç»ƒè¿™ä¸€é˜¶æ®µ
    # Python unbuffered è¾“å‡ºå·²é€šè¿‡ PYTHONUNBUFFERED=1 ç¯å¢ƒå˜é‡è®¾ç½®
    torchrun \
        --standalone \
        --nnodes 1 \
        --nproc-per-node ${NPROC} \
        --max-restarts 0 \
        vla-scripts/finetune.py \
        --vla_path "${LAST_CHECKPOINT_PATH}" \
        --data_root_dir "${DATA_ROOT_DIR}" \
        --dataset_name "${DATASET_NAME}" \
        --run_root_dir "${EXPERIMENT_DIR}" \
        --run_id_override "${RUN_ID_FIXED}" \
        --resume ${RESUME_FLAG} \
        ${RESUME_STEP_ARG} \
        --use_l1_regression True \
        --use_diffusion False \
        --use_film False \
        --num_images_in_input 2 \
        --use_proprio True \
        --batch_size ${BATCH_SIZE} \
        --grad_accumulation_steps ${GRAD_ACCUMULATION} \
        --learning_rate ${LR} \
        --lr_warmup_steps ${WARMUP_STEPS} \
        --lr_decay_milestones ${DECAY_MILESTONES} \
        --lr_decay_gamma ${DECAY_GAMMA} \
        --max_steps ${TARGET_STEP} \
        --save_freq ${SAVE_FREQ} \
        --save_latest_checkpoint_only ${SAVE_LATEST_ONLY} \
        --image_aug ${IMAGE_AUG} \
        --lora_rank ${LORA_RANK} \
        --prune_disable ${PRUNE_DISABLE} \
        --prune_coverage_warmup ${COVERAGE_WARMUP} \
        --prune_coverage_follow_min_keep ${PRUNE_COVERAGE_FOLLOW_MIN_KEEP} \
        --prune_coverage_offset ${PRUNE_COVERAGE_OFFSET} \
        --prune_prompt_aggregation ${PRUNE_AGGREGATION} \
        --prune_logsumexp_temperature ${PRUNE_LSE_TEMP} \
        --prune_soft_rescale_mean_preserve ${PRUNE_RESCALE} \
        --prune_soft_rescale_clip ${PRUNE_CLIP} \
        --prune_disable_keep_bins ${PRUNE_DISABLE_KEEP_BINS} \
        --prune_min_keep_ratio_warmup ${PRUNE_MIN_KEEP_RATIO_WARMUP} \
        --prune_min_keep_ratio_target ${PRUNE_MIN_KEEP_RATIO_TARGET} \
        --prune_min_keep_ramp_steps ${PRUNE_MIN_KEEP_RAMP_STEPS} \
        --prune_train_use_st_topk ${PRUNE_TRAIN_USE_ST_TOPK} \
        --prune_train_gumbel_tau_start ${PRUNE_TAU_START} \
        --prune_train_gumbel_tau_end ${PRUNE_TAU_END} \
        --prune_train_gumbel_tau_ramp_steps ${PRUNE_TAU_RAMP_STEPS} \
        --shuffle_buffer_size 10000 \
        --log_freq 10 2>&1 | tee -a ${LOG_FILE}
    
    TRAIN_EXIT_CODE=$?
    
    if [ ${TRAIN_EXIT_CODE} -ne 0 ]; then
        echo "âŒ è®­ç»ƒé˜¶æ®µ ${stage} å¤±è´¥ (exit code: ${TRAIN_EXIT_CODE})"
        exit ${TRAIN_EXIT_CODE}
    fi
    
    echo "âœ… é˜¶æ®µ ${stage} è®­ç»ƒå®Œæˆ"
    
    # æ‰¾åˆ°æ–°ç”Ÿæˆçš„ checkpointï¼ˆåŒ…å«æ—¶é—´æˆ³çš„ç›®å½•åï¼‰
    # finetune.py ç”Ÿæˆçš„ç›®å½•æ ¼å¼ï¼šrun_id--${step}_chkpt
    # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŒ…å« "--${TARGET_STEP}_chkpt" çš„æœ€æ–°ç›®å½•
    NEW_CHECKPOINT=$(find "${EXPERIMENT_DIR}" -maxdepth 1 -type d -name "*--${TARGET_STEP}_chkpt" | sort -r | head -1)
    
    if [ -z "${NEW_CHECKPOINT}" ] || [ ! -d "${NEW_CHECKPOINT}" ]; then
        echo "âš ï¸ æœªæ‰¾åˆ° checkpoint: *--${TARGET_STEP}_chkpt"
        echo "   åœ¨ç›®å½•: ${EXPERIMENT_DIR}"
        echo "   è·³è¿‡æœ¬æ¬¡è¯„ä¼°"
        echo ""
        echo "   å¯ç”¨çš„checkpointç›®å½•ï¼š"
        ls -ld "${EXPERIMENT_DIR}"/*chkpt 2>/dev/null || echo "   æ— "
    else
        echo "âœ… æ‰¾åˆ° checkpoint: $(basename ${NEW_CHECKPOINT})"
        
        # è¿è¡Œè¯„ä¼°
        run_evaluation "${NEW_CHECKPOINT}" "${TARGET_STEP}"
        
        # æ›´æ–°ä¸ºä¸‹ä¸€é˜¶æ®µçš„èµ·ç‚¹
        LAST_CHECKPOINT_PATH="${NEW_CHECKPOINT}"
        echo "ğŸ“ ä¸‹ä¸€é˜¶æ®µå°†ä»æ­¤checkpointç»§ç»­è®­ç»ƒ"
    fi
    
    # æ›´æ–°å½“å‰æ­¥æ•°
    CURRENT_STEP=${TARGET_STEP}
    
    echo ""
done

# ========== è®­ç»ƒç»“æŸ ==========
echo ""
echo "============================================"
echo "ğŸ‰ è®­ç»ƒ+è¯„ä¼°æµç¨‹å…¨éƒ¨å®Œæˆï¼"
echo "============================================"
echo ""
echo "ğŸ“‚ å®éªŒç›®å½•: ${EXPERIMENT_DIR}"
echo "ğŸ“ è®­ç»ƒæ—¥å¿—: ${LOG_FILE}"
echo "ğŸ“Š è¯„ä¼°æ—¥å¿—: ${EVAL_LOG_FILE}"
echo ""
echo "ğŸ’¾ Checkpoints:"
find "${EXPERIMENT_DIR}" -maxdepth 1 -type d -name "*chkpt" | sort
echo ""
echo "ğŸ“ˆ è¯„ä¼°ç»“æœæ±‡æ€»:"
if [ -f "${EVAL_LOG_FILE}" ]; then
    grep "Overall success rate" ${EVAL_LOG_FILE} || echo "æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ"
fi
echo ""
